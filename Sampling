# Japneet Khanna| 102203198 | Sampling
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from sklearn.utils import resample
import random

# Step 1: Load the dataset
url = "path_to_your_downloaded_dataset.csv"  # Replace with your file path
data = pd.read_csv(url)

# Inspect the dataset
print(data.head())
print(data.info())

# Step 2: Preprocess the dataset (Balance the classes using SMOTE as an example)
target_column = 'target'  # Replace with the correct target column name
X = data.drop(target_column, axis=1)
y = data[target_column]

# Balance the dataset using SMOTE
smote = SMOTE(random_state=42)
X_balanced, y_balanced = smote.fit_resample(X, y)

# Step 3: Create Samples
# Define a sampling size formula (replace with your own logic if required)
sample_size = int(0.2 * len(X_balanced))  # Example: 20% of the dataset

# Create five samples using different techniques
samples = []

# Sampling 1: Random Sampling
sample1 = resample(pd.concat([X_balanced, y_balanced], axis=1), n_samples=sample_size, random_state=42)
samples.append(sample1)

# Sampling 2: Stratified Sampling
from sklearn.model_selection import StratifiedShuffleSplit
sss = StratifiedShuffleSplit(n_splits=1, test_size=sample_size / len(X_balanced), random_state=42)
for train_index, sample_index in sss.split(X_balanced, y_balanced):
    sample2 = pd.concat([X_balanced.iloc[sample_index], y_balanced.iloc[sample_index]], axis=1)
samples.append(sample2)

# Sampling 3: Systematic Sampling
k = len(X_balanced) // sample_size
sample3 = pd.concat([X_balanced.iloc[::k], y_balanced.iloc[::k]], axis=1)
samples.append(sample3)

# Sampling 4: Oversampling
oversampled = resample(pd.concat([X_balanced, y_balanced], axis=1), replace=True, n_samples=sample_size, random_state=42)
samples.append(oversampled)

# Sampling 5: Undersampling
undersampler = RandomUnderSampler(sampling_strategy=0.5, random_state=42)
X_under, y_under = undersampler.fit_resample(X_balanced, y_balanced)
sample5 = pd.concat([X_under, y_under], axis=1).sample(n=sample_size, random_state=42)
samples.append(sample5)

# Step 4: Train Five ML Models
models = {
    'M1': LogisticRegression(max_iter=1000),
    'M2': DecisionTreeClassifier(random_state=42),
    'M3': RandomForestClassifier(random_state=42),
    'M4': SVC(kernel='linear', random_state=42),
    'M5': KNeighborsClassifier()
}

# Dictionary to store accuracy results
results = {}

# Evaluate models on each sample
for i, sample in enumerate(samples):
    X_sample = sample.drop(target_column, axis=1)
    y_sample = sample[target_column]
    
    X_train, X_test, y_train, y_test = train_test_split(X_sample, y_sample, test_size=0.3, random_state=42)
    
    for model_name, model in models.items():
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
        
        if model_name not in results:
            results[model_name] = []
        results[model_name].append(accuracy)

# Step 5: Output the results
results_df = pd.DataFrame(results, index=[f"Sampling{i+1}" for i in range(5)])
print(results_df)

# Step 6: Save results to CSV
results_df.to_csv("sampling_results.csv", index=True)
